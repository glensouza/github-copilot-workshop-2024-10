# LLMs and OpenAI's **Codex** That Powers GitHub Copilot

When we use GitHub Copilot, we get accurate and custom responses specific to our code project and relevant tabs within a couple of seconds. Now, it's pretty remarkable. But in order for those code suggestions to be provided, there is an incredible amount of work happening under the hood, and most of it happens in less than a blink of an eye. Now, when it comes to these large‑scale models that are responsible for generating these human‑like responses with text, code suggestions, text to speech, and even realistic images, we can quickly get very technical and reach beyond the scope of this course. So, we won't be doing a deep dive into the iterations of generative AI that are based on transformer neural network architectures because they're easier to scale to larger model sizes or the number of parameters and train using huge datasets. Now you can visit OpenAI's documentation and learn all about their diverse set of models with different capabilities, customizations, and fine‑tuning. But for this course, I think it's important to have a basic understanding of what's happening under the hood and how your prompts are being handled. So to start, let's define what LLMs are or these large language models. If you were to ask ChatGPT, you'd get an answer pretty similar to this. LLMs referred to a type of artificial intelligence model that is trained on massive amounts of text data to understand and generate human‑like language. Now these models use machine learning techniques, particularly deep learning, to process and analyze language patterns, enabling them to perform various natural language processing tasks. So basically, the important thing is that LLMs generate humanlike language, whether presented as text completion or as chat. And ultimately, the models take a prompt and generate what a human would likely say next. They do this by training on massive amounts of text pretty much the entire internet. You can think of them as a person without much personality or deductive ability, but who's read everything ever written and who has a fantastic memory. At this point, I'll guess that the most popular and widely known LLM is GPT‑3 or GPT‑4, which can understand, as well as generate natural language or code and is the LLM behind ChatGPT. Now you may have also heard about large scale models. Are those the same? Well, the terms large language models and large scale models are often used interchangeably, but they do have a nuanced difference depending on the context. Generally, they both refer to models that are characterized by having a large number of parameters and being trained on a massive dataset. But as LLMs are models designed for natural language processing tasks, focusing on understanding and generated human language or code generation, large scale models can refer to models that are not necessarily focused on language but are characterized by their sheer size and complexity. This term could include various types of models across different domains, such as image generation and editing, text to audio, or audio to text. And here is a list of OpenAI's diverse set of models with different capabilities. Models GPT‑4 and 3 are natural language and code models where DALL‑E is used to generate and edit images given a natural language prompt. You also have models like TTS and Whisper that are used for text to audio or audio to text. So, the big question is what kind of LLM or large scale model does GitHub Copilot use? And the answer is OpenAI's Codex OpenAI Codex is a descendant of GPT‑3. Its training data contains both natural language and billions of lines of source code from publicly available sources including code in public GitHub repositories. OpenAI Codex is most capable in Python, but it's also proficient in over a dozen languages including JavaScript, Go, Perl, PHP, Ruby, Swift, and TypeScript and even Shell. It has a memory of 14 KB for Python code compared to GPT‑3, which only has 4 KB. So it can take into account over 3 times as much contextual information while performing any task. OpenAI Codex has much of the natural language understanding of GPT‑3, but it produces working code, meaning you can issue commands in English to any piece of software with an API. Okay, before we dive any deeper, I think we'll stop there and summarize that the OpenAI's Codex large language model is pretty special. LLMs like OpenAI's GPT‑3, GPT‑4, and Codex models are trained on an enormous amount of natural language data and publicly available source code. This is part of the reason why tools like ChatGPT and GitHub Copilot, which are built on these models, can produce contextually accurate outputs. So before we wrap up this section, let's look at what is happening within these models. Many of these large‑scale models like GPT‑4, GPT‑3, DALL‑E, and Codex are classified as foundation models. These foundational models are trained on a broad set of data at scale, so they can be adopted to a wide range of downstream tasks. These large‑scale foundation models are called foundational because lots of specific tasks can be built on top of them due to the size of the data capability it can process. So for example, GPT‑4 is trained on both text, as well as image encodings. And DALL‑E is trained on images, but also captions and text. So with these foundation models, data is sent over to the model which is a deep neural network model called a transformer model. These transformer models take an input sequence, pass it through some encoding which then sends it to a decoder that takes it and transforms it into some output format on the other end. Now this could look like a prompt for the input side and generating an image on the output side or taking in English on the input side and generating Portuguese on the output side. Now, there is a lot more that goes on with these deep neural network transformer model. And in fact, the way that GitHub Copilot Chat works is slightly different because it works as a decode‑only model by taking in a prompt that has been embedded using tokenization that the model can understand and then providing an output using a next word prediction type of output. But again, we're quickly going beyond the scope of this course. But I do think having a high‑level understanding of what these large scale models do and how GitHub Copilot uses them is good to know. Actually, with a basic understanding of what LLMs are, next we'll spend some time going over the data flow on what happens when you interact with GitHub Copilot within your text editor.
